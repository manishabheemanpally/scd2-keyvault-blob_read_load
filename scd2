from notebookutils import mssparkutils
from azure.storage.blob import BlobServiceClient
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, xxhash64, concat_ws, current_timestamp, row_number, desc, coalesce
from pyspark.sql.types import IntegerType, BooleanType
from pyspark.sql.window import Window
from delta.tables import DeltaTable
from datetime import datetime
import io
import pandas as pd

spark = SparkSession.builder \
    .appName("SCD2_Blob_Processing") \
    .config("spark.sql.catalogImplementation", "hive") \
    .getOrCreate()

key_vault_uri = 'https://azeusprod-i2-kv.vault.azure.net/'
secret_name = 'SCPO-StorageConnectionString'
connection_string = mssparkutils.credentials.getSecret(key_vault_uri, secret_name)
container_name = "scpo-files-storage"
blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client(container_name)

table_configs = {
    "schedrcpts": {
        "primary_keys": ["item", "loc", "scheddate", "expdate", "startdate", "seqnum", "supplyid"],
        "temp_table": "schedrcpts_temp",
        "target_table": "schedrcpts"
    },
    "recship": {
        "primary_keys": ["item", "source", "dest", "type", "seqnum"],
        "temp_table": "recship_temp",
        "target_table": "recship"
    },
    "item": {
        "primary_keys": ["item"],
        "temp_table": "item_temp",
        "target_table": "item"
    },
    "vehicleloadline": {
        "primary_keys": ["loadid", "primaryitem", "expdate", "item", "dest", "supplyid"],
        "temp_table": "vehicleloadline_temp",
        "target_table": "vehicleloadline"
    }
}

def scd2_merge(target_table_name, df_source, primary_keys, load_datetime_col='LoadDateTime'):
    try:
        if not primary_keys:
            raise ValueError("Primary keys list cannot be empty.")
        missing_keys = [k for k in primary_keys if k not in df_source.columns]
        if missing_keys:
            raise ValueError(f"Primary key(s) {missing_keys} not found in source DataFrame.")
        if load_datetime_col not in df_source.columns:
            raise ValueError(f"LoadDateTime column {load_datetime_col} not found in source DataFrame.")

        all_columns = df_source.columns
        compare_columns = [c for c in all_columns if c not in primary_keys + [load_datetime_col]]
        if not compare_columns:
            pass

        window_spec = Window.partitionBy(*primary_keys).orderBy(desc(load_datetime_col))
        df_source_latest = (
            df_source
            .withColumn("row_num", row_number().over(window_spec))
            .filter("row_num = 1")
            .drop("row_num")
        )

        if spark.catalog.tableExists(target_table_name):
            delta_table = DeltaTable.forName(spark, target_table_name)
            df_target = delta_table.toDF().filter((col("active_flag") == 1) | (col("isdeleted") == True))

            join_cond = [df_source_latest[k] == df_target[k] for k in primary_keys]
            df_joined = df_source_latest.alias("src").join(df_target.alias("tgt"), join_cond, "left")

            src_cols = [f"src.{c}" for c in compare_columns]
            tgt_cols = [f"tgt.{c}" for c in compare_columns]
            df_changed = df_joined.filter(
                (col(f"tgt.{primary_keys[0]}").isNull()) |
                (col("tgt.isdeleted") == True) |
                (xxhash64(*[coalesce(col(c), lit("")) for c in src_cols]) !=
                 xxhash64(*[coalesce(col(c), lit("")) for c in tgt_cols]))
            ).select("src.*")

            if not df_changed.rdd.isEmpty():
                merge_condition = " AND ".join([f"tgt.{k} = src.{k}" for k in primary_keys])
                delta_table.alias("tgt").merge(
                    df_changed.alias("src"),
                    merge_condition
                ).whenMatchedUpdate(
                    condition="tgt.active_flag = 1 AND tgt.isdeleted = false",
                    set={
                        "active_flag": lit(0).cast(IntegerType()),
                        "isdeleted": lit(False).cast(BooleanType())
                    }
                ).execute()

                df_to_insert = (
                    df_changed
                    .withColumn("active_flag", lit(1).cast(IntegerType()))
                    .withColumn("isdeleted", lit(False).cast(BooleanType()))
                )
                df_to_insert.write.format("delta").mode("append").saveAsTable(target_table_name)

            df_not_in_source = df_target.alias("tgt") \
                .join(df_source_latest.alias("src"), on=primary_keys, how="left_anti") \
                .filter("tgt.active_flag = 1 AND tgt.isdeleted = false")

            if not df_not_in_source.rdd.isEmpty():
                merge_condition = " AND ".join([f"tgt.{k} = src.{k}" for k in primary_keys])
                delta_table.alias("tgt").merge(
                    df_not_in_source.alias("src"),
                    merge_condition
                ).whenMatchedUpdate(
                    set={
                        "active_flag": lit(0).cast(IntegerType()),
                        "isdeleted": lit(True).cast(BooleanType())
                    }
                ).execute()

        else:
            df_source_latest \
                 .withColumn("active_flag", lit(1).cast(IntegerType())) \
                 .withColumn("isdeleted", lit(False).cast(BooleanType())) \
                 .write.format("delta") \
                 .mode("overwrite") \
                 .saveAsTable(target_table_name)

        return f"SCD2 merge completed successfully for table {target_table_name} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}."

    except Exception as e:
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return f"Exception occurred while processing SCD2 on TargetTable {target_table_name}: {str(e)} at {current_time}"

log_entries = []
allowed_prefixes = set(table_configs.keys())


blobs_list = container_client.list_blobs()
for blob in blobs_list:
    try:
        blob_name = blob.name
        blob_prefix = blob_name.split("_")[0].lower()

        if blob_prefix not in allowed_prefixes:
            continue

        blob_client = container_client.get_blob_client(blob_name)
        blob_data = blob_client.download_blob().readall()
        data = pd.read_parquet(io.BytesIO(blob_data))
        
        file_record_count = data.shape[0]
        df = spark.createDataFrame(data)
        df = df.select([col(c).cast("string").alias(c.replace(" ", "_")) for c in df.columns])

        table_key = blob.name.split("_")[0].lower()
        if table_key not in table_configs:
            raise Exception(f"Table config not found for prefix: {table_key}")

        table_conf = table_configs[table_key]
        primary_keys = table_conf["primary_keys"]
        temp_table = table_conf["temp_table"]
        target_table = table_conf["target_table"]

        df = df.withColumn("HashKey", sha2(concat_ws("|", *[col(c) for c in primary_keys]), 256)) \
                .withColumn("SourceSystemId", lit(42)) \
                .withColumn("LoadDateTime", current_timestamp())

        if spark.catalog.tableExists(temp_table):
            spark.sql(f"DROP TABLE {temp_table}")
        df.write.format("delta") \
            .mode("overwrite") \
            .option("delta.enableChangeDataFeed", "true") \
            .saveAsTable(temp_table)

        source_df = spark.read.table(temp_table)
        source_table_count = source_df.count()

        result = scd2_merge(target_table, source_df, primary_keys)
        
        after_scd2_record_count = spark.read.table(target_table).count()

        blob_client.delete_blob()
        print(f"Successfully processed and deleted: {blob.name}")

        blob_size = blob.size
        if blob_size >= 1024 * 1024:
            size = blob_size / (1024 * 1024)
            unit = "MB"
        elif blob_size >= 1024:
            size = blob_size / 1024
            unit = "KB"
        else:
            size = blob_size
            unit = "bytes"
        readable_blob_size = f"{size:.2f} {unit}"

        log_entries.append({
            "blob_name": blob_name,
            "table_name": temp_table,
            "target_table": target_table,
            "status": "Success",
            "file_record_count": file_record_count,
            "source_record_count": source_table_count,
            "target_record_count": after_scd2_record_count,
            "blob_size": readable_blob_size,
            "message": result if "Exception" in result else "Data written, SCD2 applied, and blob deleted",
            "timestamp": str(datetime.now())
        })

    except Exception as e:
        log_entries.append({
            "blob_name": blob_name,
            "table_name": table_conf["temp_table"] if 'table_conf' in locals() else "Unknown",
            "target_table": table_conf["target_table"] if 'table_conf' in locals() else "Unknown",
            "status": "Failed",
            "file_record_count": file_record_count if 'file_record_count' in locals() else 0,
            "source_record_count": source_table_count if 'source_table_count' in locals() else 0,
            "target_record_count": after_scd2_record_count if 'after_scd2_record_count' in locals() else 0,
            "blob_size": readable_blob_size if 'readable_blob_size' in locals() else "0 bytes",
            "message": str(e),
            "timestamp": str(datetime.now())
        })

if log_entries:
    log_df = spark.createDataFrame(log_entries)
    log_df.write.mode("append") \
          .format("delta") \
          .option("mergeSchema", "true") \
          .saveAsTable("table_logs")
